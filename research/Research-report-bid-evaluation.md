# Evaluating Connectivity RFPs: Best Practices and Criteria

Best practices for evaluating connectivity RFP bids encompass cost-effectiveness, technical quality, and
vendor credibility, while incorporating forward-looking elements like resilience, sustainability, and social
impact, structured within a comprehensive framework from pre-evaluation through post-selection monitoring.

## Abstract

Evaluations of bids for connectivity-related RFPs consistently stress criteria that address both immediate
and long-term concerns. Several studies specify the importance of assessing net costs (with attention to
long-term cost-effectiveness), technical quality (including functionality, reliability, ease of use, and future-
proofing), and vendor credibility based on performance history. Additional criteria include resilience (the
capacity to adapt to disruptions), stakeholder engagement (ensuring transparent, inclusive decision-making),
implementation capability, innovation, customization to local needs, sustainability (covering financial and
environmental dimensions), and social impact (such as community development and improved educational
outcomes). One study in the construction sector noted a 70-percent weight on price along with three nonprice
factors, underscoring the practical balance of evaluation metrics.

Emerging evaluation methods build on these foundations. Araújo et al. (2022) advocate for integrating
option theory into decision frameworks to enhance negotiation and flexibility. Similarly, Ramezani et al.
(2019) recommend a focus on resilience that maximizes recovery in the face of disruptions. Educational
and community impact assessments, as developed by Ruiz et al. (2022), illustrate outcome-focused metrics
that align with school connectivity goals. A structured framework segments the process into key phases—
from pre-evaluation (emphasizing stakeholder engagement and criteria definition) through bid solicitation,
screening, detailed evaluation, negotiation, selection, and post-selection monitoring—to guide bid evaluation
in dynamic, diverse contexts such as connecting every school to the internet by 2030.

## Paper search

Using your research question "What are some of the best practices in evaluating bids submitted by vendors
for connectivity related RFPs? Examine especially emerging and outcome focused evaluation criteria that
is forward looking, as well as those proven to work well, from past evaluations. Give clear and concise
descriptions of the different criteria for evaluation. Context: Giga's mission to connect every school to the
internet by 2030.", we searched across over 126 million academic papers from the Semantic Scholar corpus.
We retrieved the 50 papers most relevant to the query.

## Screening

We screened in papers that met these criteria:

* **Connectivity Focus**: Does the study examine bid evaluation processes for telecommunications/internet connectivity projects OR technology infrastructure procurement?
* **Public Sector Context**: Does the research address public sector procurement practices or public benefit considerations?
* **Evaluation Framework**: Does the study include comprehensive evaluation criteria beyond just technical specifications, pricing, or regulations alone?
* **Infrastructure Context**: Does the research focus on infrastructure technology projects or school connectivity initiatives?
* **Methodology Scope**: Does the study examine vendor selection criteria, risk assessment, or outcome-based evaluation methods?
* **Evidence Synthesis**: Is the study a primary research study, systematic review, or meta-analysis of procurement evaluation practices?
* **Bid Evaluation Focus**: Does the study include analysis of initial bid evaluation or vendor selection processes (not solely post-implementation performance)?
* **Practical Application**: Does the research provide practical insights or frameworks that can be applied to connectivity procurement evaluation?

We considered all screening questions together and made a holistic judgement about whether to screen in
each paper.

## Data extraction

We asked a large language model to extract each data column below from each paper. We gave the model
the extraction instructions shown below for each column.

* **Study Design Type**:
  
  Identify the type of study methodology used:
  
  * Theoretical framework development
  * Decision-making model
  * Empirical research
  * Case study
  * Mixed methods approach
  
  Look in the methodology or introduction sections. If multiple approaches are used, list all in order of
prominence. If unclear, note "unclear" and provide a brief explanation of why.

* **Evaluation Criteria Framework**:
  
  Extract the specific evaluation criteria used in the study for bid or vendor selection.
  
  * List each criterion verbatim
  * Note the source of the criterion (e.g., expert interviews, literature review)
  * Indicate the category or domain of the criterion (e.g., technical, cost, resilience)
  
  Look in results, methodology, or discussion sections. If criteria are not explicitly listed, note "not specified".
If multiple sets of criteria are presented, list all.

* **Sector and Context of Evaluation**:
  
  Specify:
  
  * Primary sector (e.g., education, telecommunications, IT)
  * Specific sub-context (e.g., school connectivity, broadband deployment)
  * Geographic region of study
  * Type of organizations involved
  
  Prioritize information from introduction and methodology sections. If multiple contexts are discussed, list
in order of prominence.

* **Stakeholder Involvement**:
  
  Identify:
  
  * Types of stakeholders involved in the evaluation process
  * Their specific roles in the evaluation
  * Method of stakeholder engagement (e.g., interviews, advisory board)
  
  Look in methodology and discussion sections. If no explicit stakeholder involvement is mentioned, note "No
stakeholder involvement described".

* **Primary Outcomes or Recommendations**:
  
  Extract:
  
  * Key findings related to bid or vendor evaluation
  * Specific recommendations for improving evaluation processes
  * Any novel approaches or insights developed
  
  Focus on results, discussion, and conclusion sections. If multiple key findings exist, list in order of significance.
If no clear recommendations are made, note "No specific recommendations provided".

## Results

### Characteristics of Included Studies

| Study | Study Context | Evaluation Focus | Methodology | Key Findings | Full text retrieved |
|-------|---------------|------------------|-------------|--------------|---------------------|
| Araújo et al., 2022 | Telecommunications, European Union | Broadband technology selection for rural areas | Decision-making model development | Proposed a multi-criteria decision model focusing on net costs, technical quality, and credibility of candidates | Yes |
| Costa et al., 2008 | Energy sector, Portugal | Bid evaluation for electricity transmission infrastructure | Case study, Decision-making model | Developed reusable bid evaluation models using MACBETH approach, emphasizing transparency and stakeholder involvement | Yes |
| Das and Buddress, 2007 | Information Technology sector | Evaluation of prospective e-providers | Empirical research | Found that firms prioritize intangible evaluation criteria over tangible ones | No |
| Keil and Tiwana, 2006 | Information Technology sector | Evaluation of Enterprise Resource Planning (ERP) systems | Empirical research | Identified functionality and reliability as the most important criteria for evaluating ERP systems | No |
| Obeidat, 2011 | Information Technology sector | Evaluation of IT vendor services | Empirical research | Identified seven specific criteria for evaluating IT vendor services | No |
| Ramezani et al., 2019 | Information Technology sector | Evaluation of resilient IT project contractors | Mixed methods approach | Emphasized the importance of resiliency in contractor evaluation | No |
| Rincon, 2022 | Telecommunications | Connecting rural and remote areas | Decision-making model | Proposed adjusting traditional economic models for cost vs coverage analysis in rural connectivity | No |
| Ruiz et al., 2022 | Education, PK-12 | Emerging technology adoption in education | Theoretical framework development | Developed a framework for evaluating and adopting emerging technologies in education | No |
| Waara and Bröchner, 2006 | Public sector, Construction, Sweden | Contractor selection for construction projects | Empirical research | Found a typical pattern of 70% price weight with three nonprice criteria in construction contract awards | No |
| Zam and Ahmad, 2019 | Telecommunications | Telco vendor selection | Case study | Emphasized the strategic importance of vendor selection in telecommunications | No |

Based on the information we found in the papers:

* **Study contexts and sectors**:
  * Information Technology (4 studies) and Telecommunications (3 studies) were the most common sectors.
  * Other sectors included Energy, Education, Construction, and Public sector, each represented by 1 study.
  * We found specific country or region information for only 3 studies: 1 each in the European Union, Portugal, and Sweden.
  * We didn't find location information in the abstracts or available full texts for the other 7 studies.

* **Evaluation focus**:
  * Provider/vendor evaluation was the most common focus, found in 5 studies.
  * Other focuses included technology selection/adoption (2 studies), bid evaluation (1 study), system evaluation (1 study), and rural connectivity (1 study).

* **Methodology**:
  * Empirical research was the most common approach (4 studies).
  * Decision-making models were used in 3 studies.
  * Case studies were used in 2 studies.
  * Mixed methods and theoretical framework development were each used in 1 study.

* **Diversity of approaches**:
  * We didn't find any studies that used the same combination of sector, focus, and methodology, suggesting a diverse range of approaches in evaluating technology and service providers across different contexts.

## Thematic Analysis

### Core Evaluation Criteria

| Criterion Category | Description | Implementation Considerations | Success Indicators |
|-------------------|-------------|----------------------------|-------------------|
| Cost | Net costs of the solution | Consider long-term cost implications, not just initial pricing | Cost-effectiveness, value for money |
| Technical Quality | Functionality, reliability, ease of use | Assess future-proofing capabilities | Performance metrics, user satisfaction |
| Vendor Credibility | Track record, performance history | Evaluate past project success and reliability | Successful project completion rates |
| Resilience | Ability to adapt to disruptions | Consider long-term sustainability and adaptability | Project continuity in face of challenges |
| Stakeholder Engagement | Involvement of relevant parties in decision-making | Ensure transparent and inclusive processes | Stakeholder satisfaction, community buy-in |
| Implementation Capability | Ability to execute and maintain the project | Assess resources, expertise, and methodologies | Timely project delivery, quality of implementation |
| Innovation | Incorporation of emerging technologies | Evaluate potential for future technological advancements | Adoption of cutting-edge solutions |
| Customization | Ability to tailor solutions to specific needs | Consider flexibility and adaptability of proposed solutions | Fit with local requirements and constraints |
| Sustainability | Long-term viability and environmental considerations | Assess environmental impact and long-term operational sustainability | Longevity of solution, environmental metrics |
| Social Impact | Contribution to community development and education | Evaluate potential for positive societal outcomes | Improved educational outcomes, community development indicators |

From the information we extracted from the papers:

* **Criterion descriptions**:
  * We identified 15 distinct description categories across the 10 criterion categories, with each appearing once.

* **Implementation considerations**:
  * Long-term considerations and adaptability were mentioned twice.
  * 12 other considerations were mentioned once each.

* **Success indicators**:
  * We found 15 distinct indicators.
  * Each indicator was mentioned once across the 10 criterion categories.

* **Aspects covered by the criteria**:
  * Financial: cost, cost-effectiveness
  * Technical: functionality, reliability, ease of use
  * Organizational: vendor credibility, implementation capability
  * Strategic: innovation, customization
  * Social and environmental: stakeholder engagement, sustainability, social impact

* **Long-term considerations**:
  * Cost implications
  * Future-proofing capabilities
  * Long-term sustainability
  * Adaptability to future changes

* **Stakeholder involvement**:
  * As a main criterion (Stakeholder Engagement)
  * As part of implementation considerations
  * As success indicators

* **Environmental and social impacts**:
  * In the Sustainability criterion
  * In the Social Impact criterion
  * As part of success indicators for multiple criteria

### Emerging Evaluation Approaches

1. **Forward-looking assessment methods**
   * **Option theory integration**:
     * Araújo et al. (2022) propose integrating option theory into multi-criteria frameworks.
     * Enhances negotiation processes and adapts to technological changes.
     * Allows for more flexible decision-making that accounts for future uncertainties and technological evolution.
   * **Resilience-focused evaluation**:
     * Ramezani et al. (2019) emphasize assessing a contractor's ability to minimize adverse impacts of disruptions.
     * Focuses on maximizing recovery capabilities.
     * Ensures selected vendors can adapt to unforeseen challenges.

2. **Outcome-focused metrics**
   * **Educational impact assessment**:
     * Ruiz et al. (2022) developed a framework for evaluating and adopting emerging technologies in education.
     * Emphasizes the need for flexibility in technology selection to accommodate future advancements.
   * **Community development indicators**:
     * While not explicitly mentioned, several studies suggest the importance of evaluating broader community impacts.

3. **Technology evolution considerations**
   * **Future-proofing capabilities**:
     * Araújo et al. (2022) highlight the importance of assessing how "future-proof" a technology is.
     * Considers quality-of-service indicators that may become more relevant as technology evolves.
   * **Adaptability to emerging technologies**:
     * Ruiz et al. (2022) emphasize the need for flexibility in technology selection.
     * Focuses on accommodating future advancements in educational technologies.

These emerging approaches reflect a shift towards more dynamic and comprehensive evaluation methods
that consider long-term outcomes, adaptability, and the evolving nature of technology.

### Implementation Framework

| Phase | Key Activities | Best Practices | Risk Factors |
|-------|---------------|---------------|-------------|
| Pre-evaluation | Stakeholder engagement, criteria definition | Involve diverse stakeholders, align criteria with long-term goals | Overlooking key stakeholders, narrow criteria focus |
| Bid Solicitation | RFP development, vendor outreach | Clear communication of requirements, emphasis on outcome-based specifications | Ambiguous requirements, limited vendor pool |
| Initial Screening | Application of basic criteria, shortlisting | Transparent process, use of standardized evaluation tools | Bias in screening, overlooking innovative solutions |
| Detailed Evaluation | In-depth assessment of shortlisted bids | Multi-criteria analysis, consideration of both tangible and intangible factors | Overemphasis on certain criteria, lack of holistic view |
| Negotiation | Clarification of proposals, fine-tuning of terms | Use of option theory for flexibility, focus on long-term value | Losing sight of original objectives, unbalanced negotiations |
| Selection | Final decision-making, contract award | Transparent decision process, clear communication of rationale | Political influence, short-term focus |
| Post-selection | Performance monitoring, continuous evaluation | Regular assessments, adaptive management approach | Lack of follow-up, failure to adapt to changing conditions |

Based on the information extracted from the studies:

* **Process phases**:
  * The framework outlines 7 phases: pre-evaluation, bid solicitation, initial screening, detailed evaluation, negotiation, selection, and post-selection.

* **Key Activities**:
  * We identified 13 distinct key activities across the phases.
  * Each phase has 1-2 specific activities, with no repetition across phases.
  * Activities range from stakeholder engagement in early phases to performance monitoring in the final phase.

* **Best Practices**:
  * We found 12 unique best practices.
  * Clear communication and transparent processes were mentioned in 2 phases each.
  * Other practices were phase-specific, such as using option theory in negotiation and adaptive management in post-selection.

* **Risk Factors**:
  * We identified 14 distinct risk factors across the phases.
  * Each risk factor was unique to its phase, with no repetition.
  * Risks evolve from potentially overlooking stakeholders in the pre-evaluation phase to failure to adapt in the post-selection phase.

The framework suggests a progression of activities, practices, and risks that align with the evolving nature
of the evaluation process, from initial planning to final implementation and monitoring.

### Context-Specific Adaptations

1. **School connectivity considerations**
   * **Educational outcome focus**:
     * Keil and Tiwana (2006) identified functionality and reliability as key criteria for evaluating IT systems.
     * These factors could be applied to assessing educational technology solutions.
   * **Scalability and replicability**:
     * Araújo et al. (2022) emphasized the importance of future-proofing in technology selection.
     * This concept could be extended to consider scalability of connectivity solutions.
   * **Teacher and student usability**:
     * Drawing from Keil and Tiwana's (2006) emphasis on ease of use in IT system evaluations.

2. **Regional adaptation factors**
   * **Infrastructure variability**:
     * Rincon (2022) highlighted challenges of connecting rural and remote areas.
     * Suggests the need for flexible evaluation criteria to account for varying levels of existing infrastructure.
   * **Cultural and linguistic considerations**:
     * While not explicitly mentioned in the studies, the diverse contexts suggest the importance of cultural adaptability.
   * **Regulatory compliance**:
     * Waara and Bröchner (2006) noted the importance of considering local regulations in public sector contracting.

3. **Sustainability measures**
   * **Long-term financial sustainability**:
     * Araújo et al. (2022) emphasized the importance of considering net costs.
     * This could be extended to include long-term financial viability assessments.
   * **Environmental impact**:
     * While not prominently featured in the reviewed studies, long-term sustainability considerations suggest its importance.
   * **Community empowerment**:
     * Costa et al. (2008) highlighted the importance of stakeholder involvement in decision-making processes.

These context-specific adaptations highlight the need for a nuanced approach to bid evaluation that goes
beyond generic criteria to address the unique challenges and opportunities presented by school connectivity
projects in diverse global contexts.

## References

Ajay Das, and L. Buddress. "Evaluating Prospective e‐Providers: An Empirical Study," 2007.

Basl Ali Zam, and I. Ahmad. "Talaseti Telco Vendor Selection and Process." The Digital Project Management Evolution, 2019.

C. B. E. Costa, J. Lourenço, Manuel P. Chagas, and J. B. E. Costa. "Development of Reusable Bid Evaluation Models for the Portuguese Electric Transmission Company." Decision Analytics, 2008.

Fredrik Waara, and J. Bröchner. "Price and Nonprice Criteria for Contractor Selection," 2006.

I. Rincon. "The Cost and Coverage Challenge of Connecting Rural and Remote Areas." 2022 IEEE Future Networks World Forum (FNWF), 2022.

Javaneh Ramezani, Mahmoud Sadraei, and M. Nasrollahi. "Identification and Ranking of Effective Criteria in Evaluating Resilient IT Project Contractors." 2019 International Young Engineers Forum (YEF-ECE), 2019.

M. Keil, and A. Tiwana. "Relative Importance of Evaluation Criteria for Enterprise Systems: A Conjoint Study." Information Systems Journal, 2006.

Marco Araújo, L. Ekenberg, M. Danielson, and J. Confraria. "A Multi-Criteria Approach to Decision Making in Broadband Technology Selection." Group Decision and Negotiation, 2022.

Muhammad A. Obeidat. "Evaluation of Information Technology Vendor Services: An Empirical Study," 2011.

Pati Ruiz, E. Richard, Carly M. Chillmon, Zohal Shah, Adam Kurth, Andy Fekete, Kip Glazer, et al. "Emerging Technology Adoption Framework: For PK-12 Education," 2022.
